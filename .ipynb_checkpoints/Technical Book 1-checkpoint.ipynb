{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicted Supreme Court Decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Executive Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Supreme Court Database contains 4 basic databases (each in several formats).  http://scdb.wustl.edu/ \n",
    "Two databases cover the \"legacy\" period (1791-1945) and two cover the modern period (1946-2016)\n",
    "One legacy database and one modern database contain justice level data -- one line for each justice so that the information is broken out for each individual vote.  The other legacy database and the other modern database contain on line for each case.  Other variants of the modern database are available, but for determining the outcomes of individual cases, the justice level and case level databases are the obvious choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following work was necessary to create train/test sets for cross validation. I knew that ultimately my goal was to predict modern cases.  However, I wanted to be able to work with the case level or justice level database.  Running separate train-test splits on the case level and justice level datasets would have resulted in a mismatch where cases that were in the train set for the case level would be in the test set for the justice level.  Additionally, I didn't want to train on the votes of certain justices on a specific case to predict the votes of other justices on that same case.  My solution was to performa a train-test split on the modern case-level database and then divide the justice level database on the same cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the Modern Datasets.  \n",
    "df_case_mod = pd.read_csv('SCDB_2017_01_caseCentered_Citation.csv', encoding='ISO 8859-1', dtype='object')\n",
    "df_justice_mod = pd.read_csv('SCDB_2017_01_justiceCentered_Citation.csv', encoding='ISO 8859-1', dtype='object')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was not sure of my targets at this point in my project, but I knew that I wanted to separate informaton only available after a case is decided from information available prior to the decision.  That is how I chose the features and targets for the purpose of the train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['caseId', 'docketId', 'caseIssuesId', 'voteId', 'dateDecision',\n",
       "       'decisionType', 'usCite', 'sctCite', 'ledCite', 'lexisCite', 'term',\n",
       "       'naturalCourt', 'chief', 'docket', 'caseName', 'dateArgument',\n",
       "       'dateRearg', 'petitioner', 'petitionerState', 'respondent',\n",
       "       'respondentState', 'jurisdiction', 'adminAction', 'adminActionState',\n",
       "       'threeJudgeFdc', 'caseOrigin', 'caseOriginState', 'caseSource',\n",
       "       'caseSourceState', 'lcDisagreement', 'certReason', 'lcDisposition',\n",
       "       'lcDispositionDirection', 'declarationUncon', 'caseDisposition',\n",
       "       'caseDispositionUnusual', 'partyWinning', 'precedentAlteration',\n",
       "       'voteUnclear', 'issue', 'issueArea', 'decisionDirection',\n",
       "       'decisionDirectionDissent', 'authorityDecision1', 'authorityDecision2',\n",
       "       'lawType', 'lawSupp', 'lawMinor', 'majOpinWriter', 'majOpinAssigner',\n",
       "       'splitVote', 'majVotes', 'minVotes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_case_mod.columns ## The following is a list of columns -- descriptions at the source website were used to \n",
    "                    ## choose the features and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targets = ['declarationUncon', 'caseDisposition',\n",
    "       'caseDispositionUnusual', 'partyWinning', 'precedentAlteration',\n",
    "       'voteUnclear','decisionDirection',\n",
    "       'decisionDirectionDissent', 'authorityDecision1', 'authorityDecision2',\n",
    "       'lawType', 'lawSupp', 'lawMinor', 'majOpinWriter', 'majOpinAssigner',\n",
    "       'splitVote', 'majVotes', 'minVotes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_case = df_case_mod[targets]\n",
    "X_case = df_case_mod[[col for col in df_case_mod.columns if col not in targets]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['caseId', 'docketId', 'caseIssuesId', 'voteId', 'dateDecision',\n",
       "       'decisionType', 'usCite', 'sctCite', 'ledCite', 'lexisCite', 'term',\n",
       "       'naturalCourt', 'chief', 'docket', 'caseName', 'dateArgument',\n",
       "       'dateRearg', 'petitioner', 'petitionerState', 'respondent',\n",
       "       'respondentState', 'jurisdiction', 'adminAction', 'adminActionState',\n",
       "       'threeJudgeFdc', 'caseOrigin', 'caseOriginState', 'caseSource',\n",
       "       'caseSourceState', 'lcDisagreement', 'certReason', 'lcDisposition',\n",
       "       'lcDispositionDirection', 'issue', 'issueArea'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_case.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train-test split on case level modern dataset - then pulling out the caseIds and apply to justice level sets\n",
    "\n",
    "X_case_train, X_case_test, y_case_train, y_case_test = train_test_split(X_case, y_case, test_size=.2, random_state=2017)\n",
    "\n",
    "train_caseIds = X_case_train['caseId']\n",
    "test_caseIds = X_case_test['caseId']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## applying the caseIds to create and save test and train sets at the case and justice level\n",
    "\n",
    "df_justice_mod_train = df_justice_mod[df_justice_mod.caseId.isin(train_caseIds)]\n",
    "df_justice_mod_test = df_justice_mod[df_justice_mod.caseId.isin(test_caseIds)]\n",
    "\n",
    "df_justice_mod_train.to_csv('justice_train_mod.csv')\n",
    "df_justice_mod_test.to_csv('justice_test_mod.csv')\n",
    "\n",
    "df_case_mod_train = df_case_mod[df_case_mod.caseId.isin(train_caseIds)]\n",
    "df_case_mod_test = df_case_mod[df_case_mod.caseId.isin(test_caseIds)]\n",
    "\n",
    "df_case_mod_train.to_csv('case_train_mod.csv')\n",
    "df_case_mod_test.to_csv('case_test_mod.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
